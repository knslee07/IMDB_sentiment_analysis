{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. classification using perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#import spacy\n",
    "#import seaborn as sns\n",
    "# Load SpaCy's English model\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "count        50000\n",
      "unique           2\n",
      "top       positive\n",
      "freq         25000\n",
      "Name: sentiment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print(df.describe())\n",
    "#print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "print('\\n')\n",
    "# Basic statistics\n",
    "print(df['sentiment'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculating the length of each review\n",
    "df['review_length'] = df['review'].apply(len)\n",
    "\n",
    "# Plotting review length vs sentiment\n",
    "plt.scatter(df['sentiment'], df['review_length'], alpha=0.5)\n",
    "plt.title('Review Length vs Sentiment')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       review_length   sentiment_n\n",
      "count   50000.000000  50000.000000\n",
      "mean     1309.431020      0.500000\n",
      "std       989.728014      0.500005\n",
      "min        32.000000      0.000000\n",
      "25%       699.000000      0.000000\n",
      "50%       970.000000      0.500000\n",
      "75%      1590.250000      1.000000\n",
      "max     13704.000000      1.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   review         50000 non-null  object\n",
      " 1   sentiment      50000 non-null  object\n",
      " 2   review_length  50000 non-null  int64 \n",
      " 3   sentiment_n    50000 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 1.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_n\n",
       "1    25000\n",
       "0    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_n'] = df['sentiment'].replace({'negative': 0, 'positive': 1})\n",
    "df['sentiment_n'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract the reviews and the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\",\n",
       " 'positive')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = df['review'].values\n",
    "sentiments = df['sentiment'].values\n",
    "reviews[0], sentiments[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alternative train, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data from pandas\n",
    "train_df = df.sample(frac=.8, random_state=16).reset_index(drop=True)\n",
    "X_train, y_train = train_df['review'], train_df['sentiment']\n",
    "\n",
    "test_df = df.drop(train_df.index).reset_index(drop=True)\n",
    "X_test, y_test = test_df['review'], test_df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess texts\n",
    "Using SpaCy takes too long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def preprocess_text_spacy(text):\n",
    "    # Process the text with SpaCy\n",
    "    doc = nlp(text)\n",
    "    # Tokenization and Lemmatization\n",
    "    lemmatized = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    lemmatized = [word for word in lemmatized if word.isalpha() and not nlp.vocab[word].is_stop]\n",
    "    # Return the preprocessed text\n",
    "    return ' '.join(lemmatized)\n",
    "# test for small data\n",
    "small_df=df.head()\n",
    "small_df['review'].apply(preprocess_text_spacy) \n",
    "\n",
    "#df['preprocessed_reviews'] = df['review'].apply(preprocess_text) \n",
    "#preprocessed_text=[preprocess_text_spacy(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  \n",
    "# from nltk.stem import WordNetLemmatizer  \n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text_nltk(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Stemming (or you can use Lemmatization)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Combine words back into string\n",
    "    preprocessed_text = ' '.join(stemmed_tokens)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "#df['preprocessed_reviews'] = df['review'].apply(preprocess_text_nltk) \n",
    "#preprocessed_text=[preprocess_text_nltk(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfortunately, preprocessed text yields worse results.\n",
    "### CountVectorizer does the preprocessing (tokenization, removing stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text data into numerical data using Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "#X = vectorizer.fit_transform(prepreocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 101895)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Perceptron, train, & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8734\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.92      0.88      4983\n",
      "    positive       0.91      0.83      0.87      5017\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.88      0.87      0.87     10000\n",
      "weighted avg       0.88      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = Perceptron()\n",
    "\n",
    "# Train the Perceptron\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.881\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.89      0.88      4983\n",
      "    positive       0.89      0.87      0.88      5017\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "clf = Perceptron()\n",
    "\n",
    "# Train the Perceptron\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9036\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.90      0.90      4983\n",
      "    positive       0.90      0.90      0.90      5017\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using CountVectorizer with bigrams\n",
    "bi_vectorizer = CountVectorizer(ngram_range=(1, 2))  # (1, 2) means unigrams and bigrams\n",
    "X2 = bi_vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "clf2 = Perceptron()\n",
    "\n",
    "# Train the Perceptron\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf2.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9075\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.89      0.91      4983\n",
      "    positive       0.90      0.92      0.91      5017\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "clf = Perceptron()\n",
    "\n",
    "# Train the Perceptron\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.905\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90      4983\n",
      "    positive       0.90      0.92      0.91      5017\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.90      0.90     10000\n",
      "weighted avg       0.91      0.91      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using CountVectorizer with trigrams\n",
    "tri_vectorizer = CountVectorizer(ngram_range=(1, 3))  # (1, 2) means unigrams and bigrams\n",
    "X3 = tri_vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "clf3 = Perceptron()\n",
    "\n",
    "# Train the Perceptron\n",
    "clf3.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf3.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9061\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.89      0.90      4983\n",
      "    positive       0.89      0.93      0.91      5017\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "clf = Perceptron()\n",
    "\n",
    "# Train the Perceptron\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9043\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90      4983\n",
      "    positive       0.90      0.92      0.91      5017\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_vectorizer = CountVectorizer(ngram_range=(1, 4))  # (1, 2) means unigrams and bigrams\n",
    "X4 = q_vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X4, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "clf4 = Perceptron()\n",
    "\n",
    "# Train the Perceptron\n",
    "clf4.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf4.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9103\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.92      0.91      4983\n",
      "    positive       0.91      0.91      0.91      5017\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 4))\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "clf = Perceptron()\n",
    "\n",
    "# Train the Perceptron\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. use logistic regression for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['review'].values\n",
    "sentiments = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization with TF-IDF as feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KWan\\anaconda3\\envs\\llmw\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8907\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.89      0.89      4983\n",
      "    positive       0.89      0.89      0.89      5017\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()#stop_words=stopwords.words('english'), preprocessor=preprocess_text)\n",
    "# unigram\n",
    "vectorizer = CountVectorizer()\n",
    "#X = vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "## train logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9011\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90      4983\n",
      "    positive       0.90      0.91      0.90      5017\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()#stop_words=stopwords.words('english'), preprocessor=preprocess_text)\n",
    "#X = vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "## train logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KWan\\anaconda3\\envs\\llmw\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9093\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.90      0.91      4983\n",
      "    positive       0.91      0.91      0.91      5017\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# countvectorizer with bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "## train logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KWan\\anaconda3\\envs\\llmw\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8995\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90      4983\n",
      "    positive       0.89      0.91      0.90      5017\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(ngram_range=(1, 2))#stop_words=stopwords.words('english'), preprocessor=preprocess_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "X_train_tfidf = vectorizer2.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer2.transform(X_test)\n",
    "\n",
    "## train logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KWan\\anaconda3\\envs\\llmw\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9103\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91      4983\n",
      "    positive       0.91      0.92      0.91      5017\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# countvectorizer with trigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "#X_train = vectorizer.fit_transform(X_train)\n",
    "#X_test = vectorizer.transform(X_test)\n",
    "\n",
    "## train logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8918\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89      4983\n",
      "    positive       0.88      0.91      0.89      5017\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(ngram_range=(1, 3))#stop_words=stopwords.words('english'), preprocessor=preprocess_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "X_train_tfidf = vectorizer2.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer2.transform(X_test)\n",
    "\n",
    "## train logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Make predictions\n",
    "predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. industry-strength solution using HuggingFace's pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "# import pipeline\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "reviews = df['review'].values\n",
    "sentiments = df['sentiment'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, sentiments, test_size=0.2, \n",
    "                                                    random_state=16)\n",
    "# Sentiment Analysis Pipeline using bert\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "pipe= pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.5121204853057861},\n",
       " {'label': 'POSITIVE', 'score': 0.9993765950202942},\n",
       " {'label': 'POSITIVE', 'score': 0.9991896748542786},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991722106933594},\n",
       " {'label': 'POSITIVE', 'score': 0.9988238215446472}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(df['review'][0:5].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use distilbert finetuned as the model.\n",
    "model='distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipe= pipeline('sentiment-analysis', tokenizer=tokenizer, model=model)\n",
    "pipe(df['review'][0:5].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe(df['review'].tolist()) \n",
    "# pipeline does not work because some review are longer than 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3694"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_reviews=[review for review in reviews if len(review.split())>512]\n",
    "len(long_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The Cell\" is an exotic masterpiece, a dizzying trip into not only the vast mind of a serial killer, but also into one of a very talented director. This is conclusive evidence of what can be achieved if human beings unleash their uninhibited imaginations. This is boldness at work, pushing aside thoughts to fall into formulas and cliches and creating something truly magnificent. This is the best movie of the year to date.<br /><br />I\\'ve read numerous complaints about this film, anywhere from all style and no substance to poorly cast characters and bad acting. To negatively criticize this film is to miss the point. This movie may be a landmark, a tradition where future movies will hopefully follow. \"The Cell\" has just opened the door to another world of imagination. So can we slam the door in its face and tell it and its director Tarsem Singh that we don\\'t want any more? Personally, I would more than welcome another movie by Tarsem, and would love to see someone try to challenge him.<br /><br />We\\'ve all heard talk about going inside the mind of a serial killer, and yes, I do agree that the \"genre\" is a bit overworked. The 90s were full of movies trying to depict what makes serial killers tick; some of them worked, but most failed. But \"The Cell\" does not blaze down the same trail, we are given a new twist, we are physically transported into the mind and presented with nothing less than a fascinating journey of the most mysterious subject matter ever studied.<br /><br />I like how the movie does not bog us down with too much scientific jargon trying to explain how Jennifer Lopez actually gets to enter the brain of another. Instead, she just lies down on a laboratory table and is wrapped with what looks like really long Twizzlers and jaunted into another entity. \"The Cell\" wants to let you \"see\" what it\\'s all about and not \"how\" it\\'s all about, and I guess that\\'s what some people don\\'t like. True, I do like explanations with my movies, but when a movie ventures onto new ground you must let it do what it desires and simply take it in.<br /><br />I noticed how the film was very dark when it showed reality, maybe to contrast the bright visuals when inside the brain of another. Nonetheless, the set design was simply astonishing. I wouldn\\'t be surprised if this film took home a few Oscars in cinematography, best costumes, best director and the like. If it were up to me it\\'d at least get nominated for best picture.<br /><br />I\\'ve noticed that I\\'ve kind of been repeating myself. Not because there\\'s nothing else to say, but because I can\\'t stress enough how fantastic I thought \"The Cell\" was. If you walk into the movie with a very open mind and to have it taken over with wonders and an eye-popping feast then you are assured a good time. I guess this film was just a little too much for some people, writing it off as \"weird\" or \"crazy\". I am very much into psychology and the imagination of the human mind, so it was right down my alley. Leaving the theater, I heard one audience member say \"Whoever made that movie sure did a lot of good drugs.\" If so, I want what he was smoking.<br /><br />**** (out of 4)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. truncate long reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, truncation=True, padding=True)\n",
    "pred_sentiments=pipe(X_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change sentiment strings to values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [1 if sentiment == 'positive' else 0 for sentiment in y_test]\n",
    "preds=[pred['label'].lower() for pred in pred_sentiments]\n",
    "preds=[1 if sentiment == 'positive' else 0 for sentiment in preds]\n",
    "#df['sentiment'] = df['sentiment'].replace({'negative': 0, 'positive': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8935\n",
      "Precision: 0.9153005464480874\n",
      "Recall: 0.8680486346422165\n",
      "F1 Score: 0.8910485933503837\n",
      "ROC AUC Score: 0.8935868298637534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "precision = precision_score(y_test, preds)\n",
    "recall = recall_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds)\n",
    "roc_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC Score: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. treat overflowed samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9870, 0.0130], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def sentiment_analysis_with_overflow(text, stride=50, max_length=512):\n",
    "    \"\"\" This function treats overflowed text as separate examples\"\"\"\n",
    "    # Tokenize with overflow\n",
    "    tokenized_input = tokenizer(\n",
    "        text, \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        stride=stride, \n",
    "        return_overflowing_tokens=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Filter out unnecessary items for model\n",
    "    input_ids = tokenized_input[\"input_ids\"]\n",
    "    attention_mask = tokenized_input[\"attention_mask\"]\n",
    "\n",
    "    # Initialize list to store predictions\n",
    "    predictions = []\n",
    "\n",
    "    # Process each chunk\n",
    "    for i in range(input_ids.shape[0]): # input_ids.shape[0] = number of chunks after tokenization\n",
    "        outputs = model(input_ids[i].unsqueeze(0), attention_mask=attention_mask[i].unsqueeze(0))\n",
    "        prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # Convert list of tensors to a single tensor\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "    # Aggregate results (here, just taking the mean)\n",
    "    return predictions.mean(dim=0)\n",
    "\n",
    "# Example usage\n",
    "review = \"Your very long text goes here...\"\n",
    "reviews = [[\"Your very long text goes here...\", \"I love this movie!\"]]\n",
    "result = sentiment_analysis_with_overflow(review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sentiment_analysis_with_overflow([long_reviews])\n",
      "\u001b[1;32m/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentiment_analysis_with_overflow\u001b[39m(text, stride\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Tokenize with overflow\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     tokenized_input \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         text, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         stride\u001b[39m=\u001b[39mstride, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# Filter out unnecessary items for model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/KWan/Documents/kaggle/imdb/imdb.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     input_ids \u001b[39m=\u001b[39m tokenized_input[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2523\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2522\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2523\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[1;32m   2524\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2609\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2604\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2605\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2606\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2607\u001b[0m         )\n\u001b[1;32m   2608\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2609\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2610\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2611\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2612\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2613\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   2614\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[1;32m   2615\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[1;32m   2616\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2617\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2618\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2619\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2620\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2621\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2622\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2623\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2624\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[1;32m   2625\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m   2626\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2627\u001b[0m     )\n\u001b[1;32m   2628\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2630\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2631\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2648\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2800\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2790\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2791\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2792\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2793\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2797\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2798\u001b[0m )\n\u001b[0;32m-> 2800\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   2801\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2802\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2803\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m   2804\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   2805\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[1;32m   2806\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[1;32m   2807\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2808\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2809\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2810\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2811\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2812\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2813\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2814\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2815\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[1;32m   2816\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m   2817\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2818\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    422\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    423\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    427\u001b[0m )\n\u001b[0;32m--> 429\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mencode_batch(\n\u001b[1;32m    430\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    431\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m    432\u001b[0m     is_pretokenized\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[1;32m    433\u001b[0m )\n\u001b[1;32m    435\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    441\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    442\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    443\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    453\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "sentiment_analysis_with_overflow([long_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
